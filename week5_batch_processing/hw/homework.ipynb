{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779c9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/28 15:40:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f353033",
   "metadata": {},
   "source": [
    "### Q1: Install Spark and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc97f8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e98c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"fhvhv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a39ab",
   "metadata": {},
   "source": [
    "### Q2: HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "```bash\n",
    "wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
    "```\n",
    "\n",
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. We will use this dataset for all\n",
    "the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it to\n",
    "parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c53ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/raw: File exists\n",
      "mkdir: data/raw/fhvhv: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/raw\n",
    "!mkdir data/raw/fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d4f257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-28 15:46:33--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.160.9\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.160.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘./data/raw/fhvhv/fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "./data/raw/fhvhv/fh 100%[===================>] 699,83M  10,2MB/s    in 88s     \n",
      "\n",
      "2022-02-28 15:48:01 (8,00 MB/s) - ‘./data/raw/fhvhv/fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  -O ./data/raw/fhvhv/fhvhv_tripdata_2021-02.csv https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7f3951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "    types.StructField(\"hvfhs_license_num\", types.StringType(), True),\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4dd60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema)\\\n",
    "    .csv('data/raw/fhvhv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a027540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f0f4695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/28 15:53:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/28 15:53:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/28 15:53:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/28 15:53:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/28 15:53:23 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/28 15:53:28 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 63,33% for 12 writers\n",
      "22/02/28 15:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 69,09% for 11 writers\n",
      "22/02/28 15:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 76,00% for 10 writers\n",
      "22/02/28 15:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 84,44% for 9 writers\n",
      "22/02/28 15:53:32 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(24) \\\n",
    "    .write.parquet('data/pq/fhvhv/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d25e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00001-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00002-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00003-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00004-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00005-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00006-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00007-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00008-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00009-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00010-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00011-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00012-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00013-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00014-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00015-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00016-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00017-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00018-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00019-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00020-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00021-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00022-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "part-00023-bfbd0825-dfa6-4a4b-9fc8-db889928f38c-c000.snappy.parquet\n",
      "221M\tdata/pq/fhvhv/\n"
     ]
    }
   ],
   "source": [
    "# list the files in folder\n",
    "! ls data/pq/fhvhv/ \n",
    "! du -sh data/pq/fhvhv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff26fa",
   "metadata": {},
   "source": [
    "**Answer:** 221M -> 208MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030d66e",
   "metadata": {},
   "source": [
    "### Prep Steps to run SQL queries on spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7124bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv = spark.read.parquet('data/pq/fhvhv/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0902f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iremertuerk/workspace-personal/Data-Engineering-Zoomcamp-All/dt-de-zoomcamp/venv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4288aae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02864|2021-02-04 02:19:32|2021-02-04 02:30:59|          20|         220|   null|\n",
      "|           HV0003|              B02887|2021-02-02 14:23:12|2021-02-02 14:44:21|          61|          95|   null|\n",
      "|           HV0005|              B02510|2021-02-01 01:32:24|2021-02-01 01:54:28|          41|         186|   null|\n",
      "|           HV0005|              B02510|2021-02-03 13:17:38|2021-02-03 13:24:43|         198|         160|   null|\n",
      "|           HV0005|              B02510|2021-02-03 08:17:43|2021-02-03 08:43:33|         140|          68|   null|\n",
      "|           HV0003|              B02864|2021-02-02 19:01:04|2021-02-02 19:07:22|          68|          68|   null|\n",
      "|           HV0003|              B02865|2021-02-03 12:18:15|2021-02-03 12:58:27|         113|         132|   null|\n",
      "|           HV0005|              B02510|2021-02-03 15:28:34|2021-02-03 15:42:59|          40|         106|   null|\n",
      "|           HV0003|              B02875|2021-02-03 01:13:02|2021-02-03 01:19:59|         244|         243|   null|\n",
      "|           HV0003|              B02883|2021-02-01 22:46:17|2021-02-01 22:56:09|          62|          61|   null|\n",
      "|           HV0003|              B02764|2021-02-01 06:18:12|2021-02-01 06:28:14|         170|         142|   null|\n",
      "|           HV0003|              B02870|2021-02-02 13:11:38|2021-02-02 13:20:13|          15|          16|   null|\n",
      "|           HV0003|              B02872|2021-02-03 14:31:28|2021-02-03 14:47:13|          71|          35|   null|\n",
      "|           HV0003|              B02866|2021-02-01 05:42:42|2021-02-01 06:05:48|         255|         249|   null|\n",
      "|           HV0003|              B02870|2021-02-02 09:43:19|2021-02-02 09:59:39|          14|         178|   null|\n",
      "|           HV0003|              B02887|2021-02-03 13:27:18|2021-02-03 13:43:46|          74|         116|   null|\n",
      "|           HV0005|              B02510|2021-02-03 06:30:45|2021-02-03 06:42:18|         168|         126|   null|\n",
      "|           HV0003|              B02869|2021-02-02 21:08:00|2021-02-02 21:21:12|          72|          61|   null|\n",
      "|           HV0005|              B02510|2021-02-03 11:34:12|2021-02-03 11:49:15|           7|         223|   null|\n",
      "|           HV0003|              B02764|2021-02-03 08:03:15|2021-02-03 08:38:03|         129|         265|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhvhv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c282a3",
   "metadata": {},
   "source": [
    "### Q3: Count records \n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76be097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"\n",
    "SELECT\n",
    "    date_trunc('day', pickup_datetime) AS day, \n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhvhv\n",
    "WHERE\n",
    "    date_trunc('day', pickup_datetime) == '2021-02-15'\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "\n",
    "df_Q3 = spark.sql(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab17a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|                day|count(1)|\n",
      "+-------------------+--------+\n",
      "|2021-02-15 00:00:00|  367170|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Q3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5eda3c",
   "metadata": {},
   "source": [
    "**Answer:** 367170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb588a9c",
   "metadata": {},
   "source": [
    "### Q4: Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59823151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "\n",
    "def duration(pickup, dropoff):\n",
    "    return (dropoff-pickup)\n",
    "\n",
    "duration_udf = pandas_udf(duration, returnType= types.LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d77ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Q4 = df_fhvhv.withColumn(\"duration\", duration_udf(col(\"pickup_datetime\"), col(\"dropoff_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "218b321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iremertuerk/workspace-personal/Data-Engineering-Zoomcamp-All/dt-de-zoomcamp/venv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:138: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_Q4.registerTempTable('fhvhv_with_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efbbe419",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"\"\"\n",
    "SELECT\n",
    "    pickup_datetime,\n",
    "    duration\n",
    "FROM\n",
    "    fhvhv_with_duration\n",
    "ORDER BY duration DESC\n",
    "\"\"\"\n",
    "\n",
    "df_Q4 = spark.sql(q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "138dee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                           (3 + 9) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|    pickup_datetime|      duration|\n",
      "+-------------------+--------------+\n",
      "|2021-02-11 13:40:44|75540000000000|\n",
      "|2021-02-17 15:54:53|57221000000000|\n",
      "|2021-02-20 12:08:15|44039000000000|\n",
      "|2021-02-03 20:24:25|40653000000000|\n",
      "|2021-02-19 23:17:44|37577000000000|\n",
      "|2021-02-25 17:13:35|35010000000000|\n",
      "|2021-02-20 01:36:13|34806000000000|\n",
      "|2021-02-18 15:24:19|34612000000000|\n",
      "|2021-02-18 01:31:20|34555000000000|\n",
      "|2021-02-10 20:51:39|34169000000000|\n",
      "|2021-02-10 01:56:17|32476000000000|\n",
      "|2021-02-25 09:18:18|32439000000000|\n",
      "|2021-02-21 19:59:13|32223000000000|\n",
      "|2021-02-09 18:36:13|32087000000000|\n",
      "|2021-02-06 09:48:09|31447000000000|\n",
      "|2021-02-02 09:42:30|30913000000000|\n",
      "|2021-02-10 10:12:08|30856000000000|\n",
      "|2021-02-09 13:30:13|30732000000000|\n",
      "|2021-02-21 22:50:52|30660000000000|\n",
      "|2021-02-05 21:32:33|30511000000000|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Q4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39a989",
   "metadata": {},
   "source": [
    "**Answer:** \"2021-02-11\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9fb2c",
   "metadata": {},
   "source": [
    "### Q5: Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fae5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = \"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    COUNT(*) AS occurence\n",
    "FROM\n",
    "    fhvhv\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\"\n",
    "\n",
    "df_Q5 = spark.sql(q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57052d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|dispatching_base_num|occurence|\n",
      "+--------------------+---------+\n",
      "|              B02510|  3233664|\n",
      "|              B02764|   965568|\n",
      "|              B02872|   882689|\n",
      "|              B02875|   685390|\n",
      "|              B02765|   559768|\n",
      "|              B02869|   429720|\n",
      "|              B02887|   322331|\n",
      "|              B02871|   312364|\n",
      "|              B02864|   311603|\n",
      "|              B02866|   311089|\n",
      "|              B02878|   305185|\n",
      "|              B02682|   303255|\n",
      "|              B02617|   274510|\n",
      "|              B02883|   251617|\n",
      "|              B02884|   244963|\n",
      "|              B02882|   232173|\n",
      "|              B02876|   215693|\n",
      "|              B02879|   210137|\n",
      "|              B02867|   200530|\n",
      "|              B02877|   198938|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Q5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9eaf3",
   "metadata": {},
   "source": [
    "**Answer:** 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae4eda",
   "metadata": {},
   "source": [
    "### Question 6: Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8caf2",
   "metadata": {},
   "source": [
    "#### Step 0: Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "322c245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/raw/zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "075c024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-28 15:57:17--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.128.72\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.128.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘./data/raw/zones/taxi+_zone_lookup.csv’\n",
      "\n",
      "./data/raw/zones/ta 100%[===================>]  12,03K  --.-KB/s    in 0,002s  \n",
      "\n",
      "2022-02-28 15:57:17 (7,17 MB/s) - ‘./data/raw/zones/taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./data/raw/zones/taxi+_zone_lookup.csv https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb92dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/raw/zones/taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a76a4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.write.parquet('data/pq/zones', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dfe1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('data/pq/zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "486c0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvhv.registerTempTable('fhvhv')\n",
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6417c",
   "metadata": {},
   "source": [
    "#### Step1: Join the Zone data with trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "298afc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     fhvhv.pickup_datetime,\n",
    "#     fhvhv.dropoff_datetime,\n",
    "#     fhvhv.PULocationID,\n",
    "#     fhvhv.DOLocationID,\n",
    "#     fhvhv.SR_Flag,\n",
    "\n",
    "q6_1 = \"\"\"\n",
    "SELECT\n",
    "    fhvhv.hvfhs_license_num,\n",
    "    fhvhv.dispatching_base_num,    \n",
    "    pickup_zone.Zone AS pickup_zone,\n",
    "    dropoff_zone.Zone AS dropoff_zone\n",
    "FROM fhvhv\n",
    "    LEFT JOIN zones AS pickup_zone\n",
    "    on fhvhv.PULocationID = pickup_zone.locationid\n",
    "    LEFT JOIN zones AS dropoff_zone\n",
    "    on fhvhv.DOLocationID = dropoff_zone.locationid\n",
    "\"\"\"\n",
    "df_Q6 = spark.sql(q6_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "635ef419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "\n",
    "# df_Q6 = df_Q6 \\\n",
    "#     .withColumn(\"pickup_zone\",coalesce(df_Q6.pickup_zone, F.lit('Unkown'))) \\\n",
    "#     .withColumn(\"dropoff_zone\",coalesce(df_Q6.dropoff_zone, F.lit('Unkown'))) \n",
    "\n",
    "df_Q6 = df_Q6 \\\n",
    "    .na.fill(\"Unknown\",\"pickup_zone\") \\\n",
    "    .na.fill(\"Unknown\",\"dropoff_zone\") \\\n",
    "\n",
    "df_Q6 = df_Q6.withColumn('loc_pairs', F.concat_ws(' / ', F.col('pickup_zone'), F.col('dropoff_zone')))\n",
    "# df_Q6 = df_Q6.withColumn('loc_pairs', F.concat(F.col('pickup_zone'), F.lit(' / '), F.col('dropoff_zone')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d40f7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Q6.registerTempTable('df_Q6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2646b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "q6_2 = \"\"\"\n",
    "SELECT\n",
    "    loc_pairs,\n",
    "    COUNT(*) as occurence\n",
    "FROM df_Q6\n",
    "GROUP BY loc_pairs\n",
    "ORDER BY 2 DESC\n",
    "\"\"\"\n",
    "df_Q6 = spark.sql(q6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e4a2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           loc_pairs|occurence|\n",
      "+--------------------+---------+\n",
      "|East New York / E...|    45041|\n",
      "|Borough Park / Bo...|    37329|\n",
      "| Canarsie / Canarsie|    28026|\n",
      "|Crown Heights Nor...|    25976|\n",
      "|Bay Ridge / Bay R...|    17934|\n",
      "|Jackson Heights /...|    14688|\n",
      "|   Astoria / Astoria|    14688|\n",
      "|Central Harlem No...|    14481|\n",
      "|Bushwick South / ...|    14424|\n",
      "|Flatbush/Ditmas P...|    13976|\n",
      "|South Ozone Park ...|    13716|\n",
      "|Brownsville / Bro...|    12829|\n",
      "|    JFK Airport / NA|    12542|\n",
      "|Prospect-Lefferts...|    11814|\n",
      "|Forest Hills / Fo...|    11548|\n",
      "|Bushwick North / ...|    11491|\n",
      "|Bushwick South / ...|    11487|\n",
      "|Crown Heights Nor...|    11462|\n",
      "|Crown Heights Nor...|    11342|\n",
      "|Prospect-Lefferts...|    11308|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Q6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "196820bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(loc_pairs='East New York / East New York', occurence=45041)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Q6.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77468012",
   "metadata": {},
   "source": [
    "**Answer**: [Row(loc_pairs='East New York / East New York', occurence=45041)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc74bac",
   "metadata": {},
   "source": [
    "### Bonus question. Join type\n",
    "\n",
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "And how many stages your spark job has?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ffcf1",
   "metadata": {},
   "source": [
    "answer:\n",
    "\n",
    "- As we are also interested to list Unknown zone names as well, we can not use INNER JOIN\n",
    "- On the other hand, we dont care about the zones that are not exists in our trip data, therefore FULL OUTER JOIN would create more rows than we actually need\n",
    "- Our main focus is to keep all tripdata therefore LEFT JOIN should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4eaca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
